# must-read-papers-on-snn
Collect important papers about snn


| Content||
|:--|:--|
|[**1. Survey**](#l_Survey)|
|[**2. Supervised learning**](#l_Super_learn)|
|[2.1 Synaptic plasticity](#l_Synaptic) |[2.2 Surrogate gradient](#l_gradient)|
|[**3. Conversion**](#l_Conversion)|
|[**4. Unsupervised learning**](#l_Unsupervised)|



## 1. <span id="l_Survey">Survey</span>

1. **Deep learning in spiking neural networks.** Neural Networks 2019. [paper](https://arxiv.org/pdf/1804.08150.pdf)  
*Tavanaei A, Ghodrati M, Kheradpisheh S R, et al.*
2. **脉冲神经网络研究进展综述.** 控制与决策 2021. [paper](https://kns.cnki.net/kcms/detail/detail.aspx?doi=10.13195/j.kzyjc.2020.1006)  
*Hu Yifan, Li Guoqi, Wu Yujie, Deng Lei*
3. **Rethinking the performance comparison between SNNS and ANNS.** Neural Networks 2020. [paper](https://par.nsf.gov/servlets/purl/10188417)  
*Deng L, Wu Y, Hu X, et al.*


## 2. <span id="l_Super_learn">Supervised learning</span>

### 2.1 <span id="l_Synaptic">Synaptic plasticity </span>
1. **The tempotron: a neuron that learns spike timing–based decisions.** Nature neuroscience 2006. [paper](http://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf)  
*Gütig R, Sompolinsky H.*
2. **Supervised learning in spiking neural networks with ReSuMe: sequence learning, classification, and spike shifting.** Neural computation 2010. [paper](https://direct.mit.edu/neco/article/22/2/467/7529/Supervised-Learning-in-Spiking-Neural-Networks)  
*Ponulak F, Kasiński A.*
3. **Learning Real-World Stimuli by Single-Spike
Coding and Tempotron Rule.** IJCNN 2012. [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6252369)  
*Tang H, Yu Q, Tan K C.*
4. **A review of learning in biologically plausible spiking neural networks.** Neural Networks 2020. [paper](http://irep.ntu.ac.uk/id/eprint/38467/1/1213346_Taherkhani.pdf)  
*Taherkhani A, Belatreche A, Li Y, et al.*


### 2.2 <span id="l_gradient">Surrogate gradient </span>

1. **Training deep spiking neural networks using backpropagation.** Frontiers in neuroscience 2016. [paper](https://www.frontiersin.org/articles/10.3389/fnins.2016.00508/full)  
*Lee J H, Delbruck T, Pfeiffer M.*  
2. **Spatio-temporal backpropagation for training high-performance spiking neural networks.** Frontiers in neuroscience 2018. [paper](https://internal-journal.frontiersin.org/articles/10.3389/fnins.2018.00331/full)  
*Wu Y, Deng L, Li G, et al.*
3. **Surrogate gradient learning in spiking neural networks.**  IEEE Signal Processing Magazine 2019. [paper](https://arxiv.org/pdf/1901.09948.pdf)  
*Neftci E O, Mostafa H, Zenke F.*
4. **Direct training for spiking neural networks: Faster, larger, better.** AAAI 2019. [paper](https://ojs.aaai.org/index.php/AAAI/article/download/3929/3807)  
*Wu Y, Deng L, Li G, et al.*
5. **Slayer: Spike layer error reassignment in time.** NIPS 2018. [paper](https://proceedings.neurips.cc/paper/2018/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf)  
*Shrestha S B, Orchard G.*
6. **Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks.** arXiv 2020. [paper](https://arxiv.org/abs/2007.05785)  
*Wei Fang, Zhaofei Yu, Yanqi Chen, et al.*
7. **Temporal spike sequence learning via backpropagation for deep spiking neural networks.** NIPS 2020. [paper](https://arxiv.org/pdf/2002.10085.pdf)  
*Zhang W, Li P.*
8. **Brain-inspired global-local hybrid learning towards human-like intelligence.** arXiv 2020. [paper](https://arxiv.org/ftp/arxiv/papers/2006/2006.03226.pdf)  
*Wu Y, Zhao R, Zhu J, et al.*
10. **Convolutional Spiking Neural Networks for Spatio-Temporal Feature Extraction.** arXiv 2020. [paper](https://arxiv.org/pdf/2003.12346.pdf)  
*Samadzadeh A, Far F S T, Javadi A, et al.*
9. **Going Deeper With Directly-Trained Larger Spiking Neural Networks.** AAAI 2021. [paper](https://arxiv.org/pdf/2011.05280.pdf)  
*Zheng H, Wu Y, Deng L, et al.*

## 3. <span id="l_Conversion">Conversion</span>
1. **Spiking deep convolutional neural networks for energy-efficient object recognition.** IJCV 2015. [paper](https://link.springer.com/content/pdf/10.1007/s11263-014-0788-3.pdf)  
*Y Cao, Y Chen, D Khosla.*
2. **Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.** IJCNN 2015. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.2413&rep=rep1&type=pdf)  
*Diehl P U, Neil D, Binas J, et al.*
3. **Conversion of continuous-valued deep networks to efficient event-driven networks for image classification.** Frontiers in neuroscience 2017. [paper](https://internal-journal.frontiersin.org/articles/10.3389/fnins.2017.00682/full)  
*Rueckauer B, Lungu I A, Hu Y, et al.*
4. **Going deeper in spiking neural networks: VGG and residual architectures.** Frontiers in neuroscience 2019. [paper](https://www.frontiersin.org/articles/10.3389/fnins.2019.00095/full)  
*Sengupta A, Ye Y, Wang R, et al.*
5. **Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network.** CVPR 2020.[paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Han_RMP-SNN_Residual_Membrane_Potential_Neuron_for_Enabling_Deeper_High-Accuracy_and_CVPR_2020_paper.pdf)  
*Han B, Srinivasan G, Roy K.*
6. **Deep spiking neural network: Energy efficiency through time based coding.** ECCV 2020. [paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550392.pdf)  
*Han B, Roy K.*
7. **Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation.** ICLR 2020. [paper](https://arxiv.org/pdf/2005.01807.pdf)  
*Rathi N, Srinivasan G, Panda P, et al.*
8. **Exploring the connection between binary and spiking neural networks.** Frontiers in Neuroscience 2020. [paper](https://www.frontiersin.org/articles/10.3389/fnins.2020.00535/full)  
*Lu S, Sengupta A.*
9. **TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers.** arXiv 2020. [paper](https://arxiv.org/pdf/2008.04509.pdf)  
*Ho N D, Chang I J.*
10. **DIET-SNN: A Low-Latency Spiking Neural Network with Direct Input Encoding & Leakage and Threshold Optimization.** Openreview 2020. [paper](https://openreview.net/pdf?id=u_bGm5lrm72)  
*N Rathi, K Roy.*
11. **Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks.** ICLR 2021. [paper](https://arxiv.org/pdf/2103.00476.pdf)  
*Deng S, Gu S.*
12. **A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration.** ICML 2021. [paper](https://arxiv.org/pdf/2106.06984.pdf)  
*Li Y, Deng S, Dong X, et al.*

## 4. <span id="l_Unsupervised">Unsupervised learning</span>
1. **Learning temporally encoded patterns in networks of spiking neurons.** Neural Processing Letters 1997. [paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.2648&rep=rep1&type=pdf)  
*Ruf B, Schmitt M.*
2. **Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition.** IJCNN 2016. [paper](https://arxiv.org/ftp/arxiv/papers/1602/1602.01510.pdf)  
*Panda P, Roy K.*
3. **STDP-based spiking deep convolutional neural networks for object recognition.** Neural Networks 2018. [paper](https://arxiv.org/pdf/1611.01421.pdf)  
*Kheradpisheh S R, Ganjtabesh M, Thorpe S J, et al.*
4. **A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule.** Neural Networks 2020. [paper](https://arxiv.org/pdf/1812.06574.pdf)  
*Hao Y, Huang X, Dong M, et al.*
5. **Training deep spiking neural networks for energy-efficient neuromorphic computing.** ICASSP 2020. [paper](https://ieeexplore.ieee.org/abstract/document/9053914)  
*Srinivasan G, Lee C, Sengupta A, et al.*
